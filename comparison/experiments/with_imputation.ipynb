{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAR datasets: ['airfoil_MCAR.csv', 'christine_MCAR.csv', 'philippine_MCAR.csv', 'phoneme_MCAR.csv', 'wine_quality_MCAR.csv']\n",
      "MNAR datasets: ['airfoil_MNAR.csv', 'christine_MNAR.csv', 'philippine_MNAR.csv', 'phoneme_MNAR.csv', 'wine_quality_MNAR.csv']\n",
      "Datasets with missing values: ['cirrhosis.csv', 'equity.csv', 'fico.csv', 'support.csv', 'wiki.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = '../processed_data/no_miss'\n",
    "datasets = os.listdir(directory)\n",
    "datasets_MCAR = [dataset for dataset in datasets if dataset[-7] == 'C']\n",
    "datasets_MNAR = [dataset for dataset in datasets if dataset[-7] == 'N']\n",
    "\n",
    "directory = '../processed_data/yes_miss'\n",
    "datasets = os.listdir(directory)\n",
    "\n",
    "print('MCAR datasets:', datasets_MCAR)\n",
    "print('MNAR datasets:', datasets_MNAR)\n",
    "print('Datasets with missing values:', datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepareData(dataset_name, SEED):\n",
    "    data = pd.read_csv(f'../processed_data/no_miss/{dataset_name}')\n",
    "    y = data.y.values\n",
    "    X = data.drop('y', axis=1).values\n",
    "    n, dim = X.shape\n",
    "\n",
    "    if len(data.y.unique()) == 2:\n",
    "        regression = False\n",
    "    else:\n",
    "        regression = True\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                        random_state=SEED, stratify=y if not regression else None)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                      random_state=SEED, stratify=y_train if not regression else None)\n",
    "    \n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_val = imputer.transform(X_val)\n",
    "    X_test = imputer.transform(X_test)\n",
    "    \n",
    "    # Normalize and scale according to the training set\n",
    "    eps = 1e-6\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0) + eps\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_val = (X_val - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, dim, regression\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def data2Tensors(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    train_batch = int(2**np.ceil(np.log2(X_train.shape[0]//10)))\n",
    "    X_train, X_val, X_test = [torch.tensor(x, dtype=torch.float32) for x in [X_train, X_val, X_test]]\n",
    "    y_train, y_val, y_test = [torch.tensor(y, dtype=torch.float32) for y in [y_train, y_val, y_test]]\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=train_batch, shuffle=True)\n",
    "    val_loader, test_loader = [DataLoader(TensorDataset(X, y), batch_size=X.shape[0], shuffle=False) for X, y in [(X_val, y_val), (X_test, y_test)]]\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1, Seed: 1 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 10909.5\n",
      "Epoch 1000, val loss: 2887.342529296875\n",
      "Epoch 1500, val loss: 835.9053955078125\n",
      "Epoch 2000, val loss: 305.31939697265625\n",
      "Epoch 2500, val loss: 149.91946411132812\n",
      "Epoch 3000, val loss: 72.09498596191406\n",
      "Epoch 3500, val loss: 39.17677307128906\n",
      "Epoch 4000, val loss: 30.48405647277832\n",
      "Epoch 4500, val loss: 27.764240264892578\n",
      "Epoch 5000, val loss: 26.832918167114258\n",
      "\n",
      "Training results: 0.5310 (RF) | 0.5460 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 2 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 11124.78125\n",
      "Epoch 1000, val loss: 3865.17138671875\n",
      "Epoch 1500, val loss: 921.7392578125\n",
      "Epoch 2000, val loss: 432.8335876464844\n",
      "Epoch 2500, val loss: 222.0435333251953\n",
      "Epoch 3000, val loss: 103.85845947265625\n",
      "Epoch 3500, val loss: 52.07768249511719\n",
      "Epoch 4000, val loss: 31.33675765991211\n",
      "Epoch 4500, val loss: 24.93090057373047\n",
      "Epoch 5000, val loss: 22.956268310546875\n",
      "\n",
      "Training results: 0.4973 (RF) | 0.3843 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 3 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 11867.2685546875\n",
      "Epoch 1000, val loss: 4130.0712890625\n",
      "Epoch 1500, val loss: 734.4559326171875\n",
      "Epoch 2000, val loss: 352.6127624511719\n",
      "Epoch 2500, val loss: 175.37808227539062\n",
      "Epoch 3000, val loss: 97.33512115478516\n",
      "Epoch 3500, val loss: 59.18500518798828\n",
      "Epoch 4000, val loss: 38.101165771484375\n",
      "Epoch 4500, val loss: 28.6433162689209\n",
      "Epoch 5000, val loss: 26.178251266479492\n",
      "\n",
      "Training results: 0.4725 (RF) | 0.4055 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 4 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 8756.9716796875\n",
      "Epoch 1000, val loss: 1936.3863525390625\n",
      "Epoch 1500, val loss: 666.6512451171875\n",
      "Epoch 2000, val loss: 329.28118896484375\n",
      "Epoch 2500, val loss: 150.0662078857422\n",
      "Epoch 3000, val loss: 65.58920288085938\n",
      "Epoch 3500, val loss: 38.32025909423828\n",
      "Epoch 4000, val loss: 30.47715187072754\n",
      "Epoch 4500, val loss: 28.022445678710938\n",
      "Epoch 5000, val loss: 26.771570205688477\n",
      "\n",
      "Training results: 0.5314 (RF) | 0.4626 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 5 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 13158.0390625\n",
      "Epoch 1000, val loss: 5856.19970703125\n",
      "Epoch 1500, val loss: 1825.4134521484375\n",
      "Epoch 2000, val loss: 692.1610717773438\n",
      "Epoch 2500, val loss: 239.69357299804688\n",
      "Epoch 3000, val loss: 96.1498031616211\n",
      "Epoch 3500, val loss: 41.404415130615234\n",
      "Epoch 4000, val loss: 25.995973587036133\n",
      "Epoch 4500, val loss: 22.35599708557129\n",
      "Epoch 5000, val loss: 21.48774528503418\n",
      "\n",
      "Training results: 0.5512 (RF) | 0.5753 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 1 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7196 (RF) | 0.7131 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 2 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.6799 (RF) | 0.7159 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 3 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7223 (RF) | 0.7223 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 4 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7260 (RF) | 0.7103 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 5 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7103 (RF) | 0.7076 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 1 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7051 (RF) | 0.7026 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 2 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7088 (RF) | 0.6950 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 3 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7045 (RF) | 0.6838 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 4 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7036 (RF) | 0.7001 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 5 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7103 (RF) | 0.6932 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 1 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.402490496635437\n",
      "Epoch 1000, val loss: 0.36261317133903503\n",
      "Epoch 1500, val loss: 0.3427126705646515\n",
      "Epoch 2000, val loss: 0.33348366618156433\n",
      "Epoch 2500, val loss: 0.32712289690971375\n",
      "Epoch 3000, val loss: 0.3234300911426544\n",
      "Epoch 3500, val loss: 0.32002368569374084\n",
      "Epoch 4000, val loss: 0.31840381026268005\n",
      "Epoch 4500, val loss: 0.3177391588687897\n",
      "Epoch 5000, val loss: 0.316786527633667\n",
      "\n",
      "Training results: 0.7710 (RF) | 0.8194 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 2 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.3617693781852722\n",
      "Epoch 1000, val loss: 0.3176586925983429\n",
      "Epoch 1500, val loss: 0.3041469156742096\n",
      "Epoch 2000, val loss: 0.29938673973083496\n",
      "Epoch 2500, val loss: 0.2970748245716095\n",
      "Epoch 3000, val loss: 0.2942788600921631\n",
      "Epoch 3500, val loss: 0.29325902462005615\n",
      "\n",
      "Training results: 0.7329 (RF) | 0.7839 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 3 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.4014362096786499\n",
      "Epoch 1000, val loss: 0.3369898796081543\n",
      "Epoch 1500, val loss: 0.32443445920944214\n",
      "Epoch 2000, val loss: 0.3209048807621002\n",
      "Epoch 2500, val loss: 0.3195684254169464\n",
      "Epoch 3000, val loss: 0.3188301920890808\n",
      "\n",
      "Training results: 0.7420 (RF) | 0.7881 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 4 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.3755522072315216\n",
      "Epoch 1000, val loss: 0.3597347140312195\n",
      "Epoch 1500, val loss: 0.3552790880203247\n",
      "Epoch 2000, val loss: 0.35314980149269104\n",
      "\n",
      "Training results: 0.7627 (RF) | 0.7996 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 5 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.35993465781211853\n",
      "Epoch 1000, val loss: 0.3439537584781647\n",
      "Epoch 1500, val loss: 0.3390563428401947\n",
      "Epoch 2000, val loss: 0.3365047872066498\n",
      "Epoch 2500, val loss: 0.33501434326171875\n",
      "Epoch 3000, val loss: 0.3338441550731659\n",
      "Epoch 3500, val loss: 0.3319084346294403\n",
      "Epoch 4000, val loss: 0.32986384630203247\n",
      "Epoch 4500, val loss: 0.3281300961971283\n",
      "Epoch 5000, val loss: 0.32759642601013184\n",
      "\n",
      "Training results: 0.7522 (RF) | 0.7938 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 1 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 1.8921232223510742\n",
      "Epoch 1000, val loss: 0.7975608110427856\n",
      "Epoch 1500, val loss: 0.6498051285743713\n",
      "Epoch 2000, val loss: 0.6263124942779541\n",
      "Epoch 2500, val loss: 0.6146082878112793\n",
      "Epoch 3000, val loss: 0.6083875894546509\n",
      "Epoch 3500, val loss: 0.6028554439544678\n",
      "Epoch 4000, val loss: 0.5990735292434692\n",
      "\n",
      "Training results: 0.2749 (RF) | 0.3316 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 2 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 1.8905705213546753\n",
      "Epoch 1000, val loss: 0.8001313209533691\n",
      "Epoch 1500, val loss: 0.6149267554283142\n",
      "Epoch 2000, val loss: 0.5840879678726196\n",
      "\n",
      "Training results: 0.2447 (RF) | 0.2760 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 3 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 1.7612481117248535\n",
      "Epoch 1000, val loss: 0.7160482406616211\n",
      "Epoch 1500, val loss: 0.5717678070068359\n",
      "Epoch 2000, val loss: 0.5483938455581665\n",
      "Epoch 2500, val loss: 0.5358355641365051\n",
      "Epoch 3000, val loss: 0.5302638411521912\n",
      "Epoch 3500, val loss: 0.5237354040145874\n",
      "Epoch 4000, val loss: 0.516444742679596\n",
      "Epoch 4500, val loss: 0.5117383599281311\n",
      "\n",
      "Training results: 0.2477 (RF) | 0.3024 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 4 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 2.117227792739868\n",
      "Epoch 1000, val loss: 0.8307539224624634\n",
      "Epoch 1500, val loss: 0.6495153903961182\n",
      "Epoch 2000, val loss: 0.5995531678199768\n",
      "Epoch 2500, val loss: 0.5847699642181396\n",
      "Epoch 3000, val loss: 0.5723347663879395\n",
      "Epoch 3500, val loss: 0.5642917156219482\n",
      "\n",
      "Training results: 0.2881 (RF) | 0.2763 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 5 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 1.758895993232727\n",
      "Epoch 1000, val loss: 0.7198125123977661\n",
      "Epoch 1500, val loss: 0.5974183678627014\n",
      "Epoch 2000, val loss: 0.5787283778190613\n",
      "Epoch 2500, val loss: 0.569960355758667\n",
      "\n",
      "Training results: 0.2590 (RF) | 0.3056 (MLP)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time\n",
    "import time\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# MLP\n",
    "from models import BasicMLP\n",
    "from train_utils import train, getPredictions\n",
    "\n",
    "# Evaluation\n",
    "metrics = ['time', 'mse', 'mae', 'r2', 'acc', 'prec', 'rec', 'f1', 'roc_auc']\n",
    "\n",
    "# Experiment\n",
    "seeds = [int(bin(i)[2:]) for i in list(range(5))]\n",
    "results_forest = np.zeros((len(datasets_MCAR), len(seeds), len(metrics)))\n",
    "results_mlp = np.zeros((len(datasets_MCAR), len(seeds), len(metrics)))\n",
    "\n",
    "for did, mcar_dataset in enumerate(datasets_MCAR):\n",
    "    for sid, seed in enumerate(seeds):\n",
    "        # Verbose\n",
    "        print(f'Dataset: {did+1}, Seed: {sid+1} STARTING TRAINING\\n')\n",
    "\n",
    "        # Prepare data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, dim, regression = prepareData(mcar_dataset, seed)\n",
    "        train_loader, val_loader, test_loader = data2Tensors(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "        \n",
    "        if regression:\n",
    "            # Fit Random Forest\n",
    "            start_time = time.time()\n",
    "            model = RandomForestRegressor(n_estimators=1000,\n",
    "                                          max_depth=4,\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=5,\n",
    "                                          max_features='sqrt',\n",
    "                                          bootstrap=True,\n",
    "                                          random_state=seed)\n",
    "            model.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_forest[did, sid] = [training_time, mse, mae, r2, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "            # Fit MLP\n",
    "            start_time = time.time()\n",
    "            model = BasicMLP(input_dim=dim, \n",
    "                             model_layers=[2**int(np.log2(dim)+1), 2**int(np.log2(dim))],\n",
    "                             dropout_rate=0)\n",
    "            epochs = 5000\n",
    "            patience = 100\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            train(model=model, \n",
    "                  train_loader=train_loader, \n",
    "                  val_loader=val_loader, \n",
    "                  epochs=epochs, \n",
    "                  patience=patience, \n",
    "                  regression_flag=regression, \n",
    "                  device=device, \n",
    "                  seed=seed,\n",
    "                  verbose = True)\n",
    "            end_time = time.time()\n",
    "\n",
    "            y_pred, y_test = getPredictions(model, test_loader, device)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_mlp[did, sid] = [training_time, mse, mae, r2, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "            \n",
    "        else:\n",
    "            # Fit Random Forest\n",
    "            start_time = time.time()\n",
    "            model = RandomForestClassifier(n_estimators=500,\n",
    "                                           max_depth=4,\n",
    "                                           min_samples_split=2,\n",
    "                                           min_samples_leaf=5,\n",
    "                                           max_features='sqrt',\n",
    "                                           bootstrap=True,\n",
    "                                           random_state=seed)\n",
    "            model.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred)\n",
    "            rec = recall_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_forest[did, sid] = [training_time, np.nan, np.nan, np.nan, acc, prec, rec, f1, roc_auc]\n",
    "\n",
    "            # Fit MLP\n",
    "            start_time = time.time()\n",
    "            model = BasicMLP(input_dim=dim, \n",
    "                             model_layers=[2**int(np.log2(dim)+1), 2**int(np.log2(dim))],\n",
    "                             dropout_rate=0)\n",
    "            epochs = 5000\n",
    "            patience = 100\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            train(model=model, \n",
    "                  train_loader=train_loader, \n",
    "                  val_loader=val_loader, \n",
    "                  epochs=epochs, \n",
    "                  patience=patience, \n",
    "                  regression_flag=regression, \n",
    "                  device=device, \n",
    "                  seed=seed,\n",
    "                  verbose=True)\n",
    "            end_time = time.time()\n",
    "\n",
    "            y_pred, y_test = getPredictions(model, test_loader, device)\n",
    "            y_pred = ((torch.sigmoid(torch.tensor(y_pred)) >= 0.5) * 1.).cpu().numpy()\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred)\n",
    "            rec = recall_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_mlp[did, sid] = [training_time, np.nan, np.nan, np.nan, acc, prec, rec, f1, roc_auc]\n",
    "\n",
    "        print(f'\\nTraining results: {results_forest[did, sid][3] if regression else results_forest[did, sid][-1]:.4f} (RF) | {results_mlp[did, sid][3] if regression else results_mlp[did, sid][-1]:.4f} (MLP)\\n')\n",
    "        np.save('../results/raw/results_forest_MCAR.npy', results_forest)\n",
    "        np.save('../results/raw/results_mlp_MCAR.npy', results_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1, Seed: 1 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 10620.736328125\n",
      "Epoch 1000, val loss: 2837.836669921875\n",
      "Epoch 1500, val loss: 574.8975830078125\n",
      "Epoch 2000, val loss: 252.41244506835938\n",
      "Epoch 2500, val loss: 114.41757202148438\n",
      "Epoch 3000, val loss: 60.76890563964844\n",
      "Epoch 3500, val loss: 40.854766845703125\n",
      "Epoch 4000, val loss: 33.99954605102539\n",
      "Epoch 4500, val loss: 30.806556701660156\n",
      "Epoch 5000, val loss: 29.316131591796875\n",
      "\n",
      "Training results: 0.5638 (RF) | 0.5086 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 2 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 11234.2138671875\n",
      "Epoch 1000, val loss: 4195.4697265625\n",
      "Epoch 1500, val loss: 1195.6588134765625\n",
      "Epoch 2000, val loss: 547.4733276367188\n",
      "Epoch 2500, val loss: 231.5416717529297\n",
      "Epoch 3000, val loss: 96.56597137451172\n",
      "Epoch 3500, val loss: 43.486045837402344\n",
      "Epoch 4000, val loss: 26.11496925354004\n",
      "Epoch 4500, val loss: 21.594314575195312\n",
      "Epoch 5000, val loss: 20.494321823120117\n",
      "\n",
      "Training results: 0.5629 (RF) | 0.4517 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 3 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 11835.1845703125\n",
      "Epoch 1000, val loss: 4246.94287109375\n",
      "Epoch 1500, val loss: 731.5772705078125\n",
      "Epoch 2000, val loss: 317.9738464355469\n",
      "Epoch 2500, val loss: 157.08778381347656\n",
      "Epoch 3000, val loss: 84.81400299072266\n",
      "Epoch 3500, val loss: 49.603271484375\n",
      "Epoch 4000, val loss: 33.695770263671875\n",
      "Epoch 4500, val loss: 27.90764045715332\n",
      "Epoch 5000, val loss: 25.546520233154297\n",
      "\n",
      "Training results: 0.5538 (RF) | 0.4797 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 4 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 9018.40625\n",
      "Epoch 1000, val loss: 2336.769775390625\n",
      "Epoch 1500, val loss: 870.2673950195312\n",
      "Epoch 2000, val loss: 333.6096496582031\n",
      "Epoch 2500, val loss: 132.31495666503906\n",
      "Epoch 3000, val loss: 57.616661071777344\n",
      "Epoch 3500, val loss: 32.099300384521484\n",
      "Epoch 4000, val loss: 26.185449600219727\n",
      "Epoch 4500, val loss: 24.630184173583984\n",
      "Epoch 5000, val loss: 23.61871910095215\n",
      "\n",
      "Training results: 0.5451 (RF) | 0.5284 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 5 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 13049.451171875\n",
      "Epoch 1000, val loss: 5623.75927734375\n",
      "Epoch 1500, val loss: 1526.8306884765625\n",
      "Epoch 2000, val loss: 613.7467651367188\n",
      "Epoch 2500, val loss: 225.80006408691406\n",
      "Epoch 3000, val loss: 86.30569458007812\n",
      "Epoch 3500, val loss: 38.794105529785156\n",
      "Epoch 4000, val loss: 23.848876953125\n",
      "Epoch 4500, val loss: 19.615896224975586\n",
      "Epoch 5000, val loss: 18.136381149291992\n",
      "\n",
      "Training results: 0.5522 (RF) | 0.6119 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 1 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7205 (RF) | 0.7002 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 2 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.6928 (RF) | 0.6974 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 3 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7279 (RF) | 0.7214 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 4 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7168 (RF) | 0.7057 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 5 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7196 (RF) | 0.7140 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 1 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7163 (RF) | 0.7138 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 2 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7199 (RF) | 0.7087 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 3 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7062 (RF) | 0.6915 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 4 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7156 (RF) | 0.7164 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 5 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7248 (RF) | 0.7061 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 1 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.3958803415298462\n",
      "Epoch 1000, val loss: 0.3540281057357788\n",
      "Epoch 1500, val loss: 0.3275146782398224\n",
      "Epoch 2000, val loss: 0.31096890568733215\n",
      "Epoch 2500, val loss: 0.30429768562316895\n",
      "Epoch 3000, val loss: 0.3014680743217468\n",
      "Epoch 3500, val loss: 0.29927846789360046\n",
      "Epoch 4000, val loss: 0.2971666157245636\n",
      "Epoch 4500, val loss: 0.29594311118125916\n",
      "Epoch 5000, val loss: 0.2947891056537628\n",
      "\n",
      "Training results: 0.7718 (RF) | 0.8079 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 2 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.352994829416275\n",
      "Epoch 1000, val loss: 0.3205507695674896\n",
      "Epoch 1500, val loss: 0.3047013282775879\n",
      "Epoch 2000, val loss: 0.29326102137565613\n",
      "Epoch 2500, val loss: 0.2850510776042938\n",
      "Epoch 3000, val loss: 0.27895739674568176\n",
      "Epoch 3500, val loss: 0.2757750451564789\n",
      "Epoch 4000, val loss: 0.2727770209312439\n",
      "Epoch 4500, val loss: 0.2715708017349243\n",
      "Epoch 5000, val loss: 0.2706633508205414\n",
      "\n",
      "Training results: 0.7679 (RF) | 0.8178 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 3 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.3732272684574127\n",
      "Epoch 1000, val loss: 0.3451557159423828\n",
      "Epoch 1500, val loss: 0.3298267424106598\n",
      "Epoch 2000, val loss: 0.320949524641037\n",
      "Epoch 2500, val loss: 0.31435003876686096\n",
      "Epoch 3000, val loss: 0.310432106256485\n",
      "Epoch 3500, val loss: 0.30864080786705017\n",
      "Epoch 4000, val loss: 0.3067411780357361\n",
      "Epoch 4500, val loss: 0.3049715459346771\n",
      "Epoch 5000, val loss: 0.3042312562465668\n",
      "\n",
      "Training results: 0.7501 (RF) | 0.8007 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 4 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.3682920038700104\n",
      "Epoch 1000, val loss: 0.32663190364837646\n",
      "Epoch 1500, val loss: 0.31056302785873413\n",
      "Epoch 2000, val loss: 0.3039162755012512\n",
      "Epoch 2500, val loss: 0.3000029921531677\n",
      "Epoch 3000, val loss: 0.2974836230278015\n",
      "Epoch 3500, val loss: 0.29547250270843506\n",
      "Epoch 4000, val loss: 0.29377418756484985\n",
      "Epoch 4500, val loss: 0.29236942529678345\n",
      "Epoch 5000, val loss: 0.2914462387561798\n",
      "\n",
      "Training results: 0.7681 (RF) | 0.8166 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 5 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.4054151177406311\n",
      "Epoch 1000, val loss: 0.36605513095855713\n",
      "Epoch 1500, val loss: 0.342439204454422\n",
      "Epoch 2000, val loss: 0.33049267530441284\n",
      "Epoch 2500, val loss: 0.32280927896499634\n",
      "Epoch 3000, val loss: 0.31830114126205444\n",
      "Epoch 3500, val loss: 0.31640899181365967\n",
      "Epoch 4000, val loss: 0.3149576187133789\n",
      "Epoch 4500, val loss: 0.31292274594306946\n",
      "Epoch 5000, val loss: 0.31124740839004517\n",
      "\n",
      "Training results: 0.7674 (RF) | 0.8108 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 1 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 1.7062047719955444\n",
      "Epoch 1000, val loss: 0.8042795658111572\n",
      "Epoch 1500, val loss: 0.6381447315216064\n",
      "Epoch 2000, val loss: 0.614319920539856\n",
      "Epoch 2500, val loss: 0.6057136058807373\n",
      "Epoch 3000, val loss: 0.6014191508293152\n",
      "\n",
      "Training results: 0.2841 (RF) | 0.2629 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 2 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 1.76144278049469\n",
      "Epoch 1000, val loss: 0.8416646718978882\n",
      "Epoch 1500, val loss: 0.6428172588348389\n",
      "Epoch 2000, val loss: 0.6004022359848022\n",
      "Epoch 2500, val loss: 0.5827019214630127\n",
      "Epoch 3000, val loss: 0.5734285116195679\n",
      "Epoch 3500, val loss: 0.566923201084137\n",
      "Epoch 4000, val loss: 0.5609645247459412\n",
      "Epoch 4500, val loss: 0.5563367605209351\n",
      "\n",
      "Training results: 0.2647 (RF) | 0.2851 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 3 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 1.9362460374832153\n",
      "Epoch 1000, val loss: 0.8464472889900208\n",
      "Epoch 1500, val loss: 0.595575749874115\n",
      "Epoch 2000, val loss: 0.5432272553443909\n",
      "Epoch 2500, val loss: 0.5259457230567932\n",
      "Epoch 3000, val loss: 0.5167767405509949\n",
      "Epoch 3500, val loss: 0.5114442706108093\n",
      "Epoch 4000, val loss: 0.5082576870918274\n",
      "\n",
      "Training results: 0.2688 (RF) | 0.3116 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 4 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 1.9714536666870117\n",
      "Epoch 1000, val loss: 0.9868786334991455\n",
      "Epoch 1500, val loss: 0.672008216381073\n",
      "Epoch 2000, val loss: 0.6015328168869019\n",
      "Epoch 2500, val loss: 0.5751240253448486\n",
      "Epoch 3000, val loss: 0.5586416125297546\n",
      "Epoch 3500, val loss: 0.5506687164306641\n",
      "\n",
      "Training results: 0.3028 (RF) | 0.2962 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 5 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 1.5535322427749634\n",
      "Epoch 1000, val loss: 0.7550902366638184\n",
      "Epoch 1500, val loss: 0.5913097858428955\n",
      "Epoch 2000, val loss: 0.5510767102241516\n",
      "Epoch 2500, val loss: 0.5394815802574158\n",
      "Epoch 3000, val loss: 0.5313910841941833\n",
      "Epoch 3500, val loss: 0.5260679721832275\n",
      "\n",
      "Training results: 0.2800 (RF) | 0.3197 (MLP)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time\n",
    "import time\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# MLP\n",
    "from models import BasicMLP\n",
    "from train_utils import train, getPredictions\n",
    "\n",
    "# Evaluation\n",
    "metrics = ['time', 'mse', 'mae', 'r2', 'acc', 'prec', 'rec', 'f1', 'roc_auc']\n",
    "\n",
    "# Experiment\n",
    "seeds = [int(bin(i)[2:]) for i in list(range(5))]\n",
    "results_forest = np.zeros((len(datasets_MNAR), len(seeds), len(metrics)))\n",
    "results_mlp = np.zeros((len(datasets_MNAR), len(seeds), len(metrics)))\n",
    "\n",
    "for did, mnar_dataset in enumerate(datasets_MNAR):\n",
    "    for sid, seed in enumerate(seeds):\n",
    "        # Verbose\n",
    "        print(f'Dataset: {did+1}, Seed: {sid+1} STARTING TRAINING\\n')\n",
    "\n",
    "        # Prepare data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, dim, regression = prepareData(mnar_dataset, seed)\n",
    "        train_loader, val_loader, test_loader = data2Tensors(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "        \n",
    "        if regression:\n",
    "            # Fit Random Forest\n",
    "            start_time = time.time()\n",
    "            model = RandomForestRegressor(n_estimators=1000,\n",
    "                                          max_depth=4,\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=5,\n",
    "                                          max_features='sqrt',\n",
    "                                          bootstrap=True,\n",
    "                                          random_state=seed)\n",
    "            model.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_forest[did, sid] = [training_time, mse, mae, r2, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "            # Fit MLP\n",
    "            start_time = time.time()\n",
    "            model = BasicMLP(input_dim=dim, \n",
    "                             model_layers=[2**int(np.log2(dim)+1), 2**int(np.log2(dim))],\n",
    "                             dropout_rate=0)\n",
    "            epochs = 5000\n",
    "            patience = 100\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            train(model=model, \n",
    "                  train_loader=train_loader, \n",
    "                  val_loader=val_loader, \n",
    "                  epochs=epochs, \n",
    "                  patience=patience, \n",
    "                  regression_flag=regression, \n",
    "                  device=device, \n",
    "                  seed=seed,\n",
    "                  verbose = True)\n",
    "            end_time = time.time()\n",
    "\n",
    "            y_pred, y_test = getPredictions(model, test_loader, device)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_mlp[did, sid] = [training_time, mse, mae, r2, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "            \n",
    "        else:\n",
    "            # Fit Random Forest\n",
    "            start_time = time.time()\n",
    "            model = RandomForestClassifier(n_estimators=500,\n",
    "                                           max_depth=4,\n",
    "                                           min_samples_split=2,\n",
    "                                           min_samples_leaf=5,\n",
    "                                           max_features='sqrt',\n",
    "                                           bootstrap=True,\n",
    "                                           random_state=seed)\n",
    "            model.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred)\n",
    "            rec = recall_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_forest[did, sid] = [training_time, np.nan, np.nan, np.nan, acc, prec, rec, f1, roc_auc]\n",
    "\n",
    "            # Fit MLP\n",
    "            start_time = time.time()\n",
    "            model = BasicMLP(input_dim=dim, \n",
    "                             model_layers=[2**int(np.log2(dim)+1), 2**int(np.log2(dim))],\n",
    "                             dropout_rate=0)\n",
    "            epochs = 5000\n",
    "            patience = 100\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            train(model=model, \n",
    "                  train_loader=train_loader, \n",
    "                  val_loader=val_loader, \n",
    "                  epochs=epochs, \n",
    "                  patience=patience, \n",
    "                  regression_flag=regression, \n",
    "                  device=device, \n",
    "                  seed=seed,\n",
    "                  verbose=True)\n",
    "            end_time = time.time()\n",
    "\n",
    "            y_pred, y_test = getPredictions(model, test_loader, device)\n",
    "            y_pred = ((torch.sigmoid(torch.tensor(y_pred)) >= 0.5) * 1.).cpu().numpy()\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred)\n",
    "            rec = recall_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_mlp[did, sid] = [training_time, np.nan, np.nan, np.nan, acc, prec, rec, f1, roc_auc]\n",
    "\n",
    "        print(f'\\nTraining results: {results_forest[did, sid][3] if regression else results_forest[did, sid][-1]:.4f} (RF) | {results_mlp[did, sid][3] if regression else results_mlp[did, sid][-1]:.4f} (MLP)\\n')\n",
    "        np.save('../results/raw/results_forest_MNAR.npy', results_forest)\n",
    "        np.save('../results/raw/results_mlp_MNAR.npy', results_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL MISSINGNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepareData(dataset_name, SEED):\n",
    "    data = pd.read_csv(f'../processed_data/yes_miss/{dataset_name}')\n",
    "    y = data.y.values\n",
    "    X = data.drop('y', axis=1).values\n",
    "    n, dim = X.shape\n",
    "\n",
    "    if len(data.y.unique()) == 2:\n",
    "        regression = False\n",
    "    else:\n",
    "        regression = True\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                        random_state=SEED, stratify=y if not regression else None)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                      random_state=SEED, stratify=y_train if not regression else None)\n",
    "    \n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_val = imputer.transform(X_val)\n",
    "    X_test = imputer.transform(X_test)\n",
    "    \n",
    "    # Normalize and scale according to the training set\n",
    "    eps = 1e-6\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0) + eps\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_val = (X_val - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, dim, regression\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def data2Tensors(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    train_batch = int(2**np.ceil(np.log2(X_train.shape[0]//10)))\n",
    "    X_train, X_val, X_test = [torch.tensor(x, dtype=torch.float32) for x in [X_train, X_val, X_test]]\n",
    "    y_train, y_val, y_test = [torch.tensor(y, dtype=torch.float32) for y in [y_train, y_val, y_test]]\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=train_batch, shuffle=True)\n",
    "    val_loader, test_loader = [DataLoader(TensorDataset(X, y), batch_size=X.shape[0], shuffle=False) for X, y in [(X_val, y_val), (X_test, y_test)]]\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1, Seed: 1 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.5520844459533691\n",
      "\n",
      "Training results: 0.7377 (RF) | 0.7171 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 2 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.5043120980262756\n",
      "\n",
      "Training results: 0.6734 (RF) | 0.6991 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 3 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7146 (RF) | 0.7379 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 4 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.5612610578536987\n",
      "\n",
      "Training results: 0.7300 (RF) | 0.7429 (MLP)\n",
      "\n",
      "Dataset: 1, Seed: 5 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.5596283078193665\n",
      "\n",
      "Training results: 0.7532 (RF) | 0.7378 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 1 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.5801419615745544\n",
      "Epoch 1000, val loss: 0.5317674875259399\n",
      "Epoch 1500, val loss: 0.513891339302063\n",
      "Epoch 2000, val loss: 0.5071946382522583\n",
      "\n",
      "Training results: 0.6003 (RF) | 0.8047 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 2 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.6401071548461914\n",
      "Epoch 1000, val loss: 0.6046442985534668\n",
      "Epoch 1500, val loss: 0.5822026133537292\n",
      "Epoch 2000, val loss: 0.5624470114707947\n",
      "Epoch 2500, val loss: 0.5488713383674622\n",
      "\n",
      "Training results: 0.5756 (RF) | 0.8168 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 3 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.6177448034286499\n",
      "Epoch 1000, val loss: 0.5601268410682678\n",
      "Epoch 1500, val loss: 0.5249602794647217\n",
      "Epoch 2000, val loss: 0.5005468130111694\n",
      "\n",
      "Training results: 0.5809 (RF) | 0.7853 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 4 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.6380327343940735\n",
      "Epoch 1000, val loss: 0.5782073140144348\n",
      "Epoch 1500, val loss: 0.54960697889328\n",
      "Epoch 2000, val loss: 0.5319851636886597\n",
      "\n",
      "Training results: 0.5877 (RF) | 0.8121 (MLP)\n",
      "\n",
      "Dataset: 2, Seed: 5 STARTING TRAINING\n",
      "\n",
      "Epoch 500, val loss: 0.5967438817024231\n",
      "Epoch 1000, val loss: 0.5329652428627014\n",
      "Epoch 1500, val loss: 0.5077657103538513\n",
      "Epoch 2000, val loss: 0.49727433919906616\n",
      "\n",
      "Training results: 0.6134 (RF) | 0.8126 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 1 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7159 (RF) | 0.7316 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 2 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7374 (RF) | 0.7364 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 3 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7233 (RF) | 0.7287 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 4 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7373 (RF) | 0.7333 (MLP)\n",
      "\n",
      "Dataset: 3, Seed: 5 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7364 (RF) | 0.7326 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 1 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.8981 (RF) | 0.7965 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 2 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.8832 (RF) | 0.8232 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 3 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.8682 (RF) | 0.8644 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 4 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.8529 (RF) | 0.7889 (MLP)\n",
      "\n",
      "Dataset: 4, Seed: 5 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.8602 (RF) | 0.8340 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 1 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.8152 (RF) | 0.8397 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 2 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.8181 (RF) | 0.8146 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 3 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7735 (RF) | 0.8120 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 4 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.8408 (RF) | 0.8298 (MLP)\n",
      "\n",
      "Dataset: 5, Seed: 5 STARTING TRAINING\n",
      "\n",
      "\n",
      "Training results: 0.7837 (RF) | 0.8438 (MLP)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time\n",
    "import time\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# MLP\n",
    "from models import BasicMLP\n",
    "from train_utils import train, getPredictions\n",
    "\n",
    "# Evaluation\n",
    "metrics = ['time', 'mse', 'mae', 'r2', 'acc', 'prec', 'rec', 'f1', 'roc_auc']\n",
    "\n",
    "# Experiment\n",
    "seeds = [int(bin(i)[2:]) for i in list(range(5))]\n",
    "results_forest = np.zeros((len(datasets), len(seeds), len(metrics)))\n",
    "results_mlp = np.zeros((len(datasets), len(seeds), len(metrics)))\n",
    "\n",
    "for did, dataset in enumerate(datasets):\n",
    "    for sid, seed in enumerate(seeds):\n",
    "        # Verbose\n",
    "        print(f'Dataset: {did+1}, Seed: {sid+1} STARTING TRAINING\\n')\n",
    "\n",
    "        # Prepare data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, dim, regression = prepareData(dataset, seed)\n",
    "        train_loader, val_loader, test_loader = data2Tensors(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "        \n",
    "        if regression:\n",
    "            # Fit Random Forest\n",
    "            start_time = time.time()\n",
    "            model = RandomForestRegressor(n_estimators=1000,\n",
    "                                          max_depth=4,\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=5,\n",
    "                                          max_features='sqrt',\n",
    "                                          bootstrap=True,\n",
    "                                          random_state=seed)\n",
    "            model.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_forest[did, sid] = [training_time, mse, mae, r2, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "            # Fit MLP\n",
    "            start_time = time.time()\n",
    "            model = BasicMLP(input_dim=dim, \n",
    "                             model_layers=[2**int(np.log2(dim)+1), 2**int(np.log2(dim))],\n",
    "                             dropout_rate=0)\n",
    "            epochs = 5000\n",
    "            patience = 100\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            train(model=model, \n",
    "                  train_loader=train_loader, \n",
    "                  val_loader=val_loader, \n",
    "                  epochs=epochs, \n",
    "                  patience=patience, \n",
    "                  regression_flag=regression, \n",
    "                  device=device, \n",
    "                  seed=seed,\n",
    "                  verbose = True)\n",
    "            end_time = time.time()\n",
    "\n",
    "            y_pred, y_test = getPredictions(model, test_loader, device)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_mlp[did, sid] = [training_time, mse, mae, r2, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "            \n",
    "        else:\n",
    "            # Fit Random Forest\n",
    "            start_time = time.time()\n",
    "            model = RandomForestClassifier(n_estimators=500,\n",
    "                                           max_depth=4,\n",
    "                                           min_samples_split=2,\n",
    "                                           min_samples_leaf=5,\n",
    "                                           max_features='sqrt',\n",
    "                                           bootstrap=True,\n",
    "                                           random_state=seed)\n",
    "            model.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred)\n",
    "            rec = recall_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_forest[did, sid] = [training_time, np.nan, np.nan, np.nan, acc, prec, rec, f1, roc_auc]\n",
    "\n",
    "            # Fit MLP\n",
    "            start_time = time.time()\n",
    "            model = BasicMLP(input_dim=dim, \n",
    "                             model_layers=[2**int(np.log2(dim)+1), 2**int(np.log2(dim))],\n",
    "                             dropout_rate=0)\n",
    "            epochs = 5000\n",
    "            patience = 100\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            train(model=model, \n",
    "                  train_loader=train_loader, \n",
    "                  val_loader=val_loader, \n",
    "                  epochs=epochs, \n",
    "                  patience=patience, \n",
    "                  regression_flag=regression, \n",
    "                  device=device, \n",
    "                  seed=seed,\n",
    "                  verbose=True)\n",
    "            end_time = time.time()\n",
    "\n",
    "            y_pred, y_test = getPredictions(model, test_loader, device)\n",
    "            y_pred = ((torch.sigmoid(torch.tensor(y_pred)) >= 0.5) * 1.).cpu().numpy()\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred)\n",
    "            rec = recall_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "            training_time = end_time - start_time\n",
    "            results_mlp[did, sid] = [training_time, np.nan, np.nan, np.nan, acc, prec, rec, f1, roc_auc]\n",
    "\n",
    "        print(f'\\nTraining results: {results_forest[did, sid][3] if regression else results_forest[did, sid][-1]:.4f} (RF) | {results_mlp[did, sid][3] if regression else results_mlp[did, sid][-1]:.4f} (MLP)\\n')\n",
    "        np.save('../results/raw/results_forest_real.npy', results_forest)\n",
    "        np.save('../results/raw/results_mlp_real.npy', results_mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
